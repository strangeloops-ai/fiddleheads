{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5a56d7-13b9-4a18-a150-23f79b3179aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece]\n",
    "!pip install tf-keras\n",
    "#!pip install tensorflow[and-cuda] --to train with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7b043e-2284-4e6e-831d-75b38eea15d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.email \"patrickyamin@gmail.com\"\n",
    "!git config --global user.name \"pyamin1878\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15203aa8-d2d6-4039-a9aa-f0b0406049ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b41733-a56b-4fe2-977f-0bdafc50a6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"juancopi81/mutopia_guitar_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8263f4-4a80-4161-a149-874d92133d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab803ad-c60e-4109-a2a1-74d8728efdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sample_num = random.randint(0, len(dataset[\"train\"]))\n",
    "print(f\"{dataset['train'][sample_num]['text'][:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038d6916-ae32-48aa-9966-917f1de73f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "context_length = 256\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"juancopi81/mutopia_guitar_dataset_tokenizer\")\n",
    "\n",
    "outputs = tokenizer(\n",
    "    dataset[\"test\"][:2][\"text\"],\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13629339-d708-473b-83b6-4dbd07134e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    removed_elements_counter = 0\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "        else:\n",
    "            removed_elements_counter += 1\n",
    "    print(f\"Removed chunks with size less than context_size: {removed_elements_counter}\")\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize, batched=True, remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133348f8-2f6d-4830-94c5-0b4f594c7ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67924b27-a50f-4d9f-a50b-f3b08d759cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFGPT2LMHeadModel(config)\n",
    "model(model.dummy_inputs) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ef434-a2c9-4bc5-9e1b-0ed0a3893ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba5ca6-03cd-457a-a1f5-66ac3aa7e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = data_collator([tokenized_datasets[\"test\"][i] for i in range(5)])\n",
    "\n",
    "for key in out:\n",
    "  print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783c333a-ba46-4534-9ac9-f130313473b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(out[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f61d3f7-8785-4ad1-a77b-5baed64b8a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "tf_eval_dataset = tokenized_datasets[\"test\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "tf_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f370bed6-e780-4de1-90ff-aea784333da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU') # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c87e854-7a99-41fb-8256-a07bf80c3619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "num_epochs = 10\n",
    "num_train_steps = len(tf_train_dataset) * num_epochs\n",
    "\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=5e-5,\n",
    "    num_warmup_steps=1_000,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    ")\n",
    "\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26516e6-652f-45a8-82dd-5fa880b30537",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
